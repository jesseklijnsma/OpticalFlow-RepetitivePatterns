{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from mmeval import EndPointError\n",
    "\n",
    "from common.kitti import load_kitti_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb382544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_eval(kitti_path: str, pred_path: str):\n",
    "    gt_map: dict[int, tuple[np.ndarray, np.ndarray]] = {}\n",
    "    pred_map: dict[str, dict[int, str]] = {} # grouped by model name, then by index. Stores filenames\n",
    "\n",
    "    files = os.listdir(os.path.join(kitti_path, \"flow_occ\"))\n",
    "    pattern=re.compile(r'^(\\d{6})_10\\.png$')\n",
    "    for filename in files:\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "\n",
    "            gt_flow, gt_valid = load_kitti_flow(os.path.join(kitti_path, \"flow_occ\", filename))\n",
    "            gt_map[index] = (gt_flow, gt_valid)\n",
    "    \n",
    "    files = os.listdir(pred_path)\n",
    "    pattern = re.compile(r'^(.*)-(\\d{7})\\.png$')\n",
    "    for filename in files:\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            index = int(match.group(2))\n",
    "            model_name = match.group(1)\n",
    "\n",
    "            if model_name not in pred_map:\n",
    "                pred_map[model_name] = {}\n",
    "\n",
    "            pred_map[model_name][index] = os.path.join(pred_path, filename)\n",
    "            \n",
    "    results = []\n",
    "    for model_name, model_pred_map in tqdm(pred_map.items()):\n",
    "        \n",
    "        model, _, ckpt = model_name.rpartition('_')\n",
    "        \n",
    "        missing = set(gt_map.keys()) - set(model_pred_map.keys())\n",
    "        if missing:\n",
    "            print(model_name, 'is missing predictions for:', missing)\n",
    "        \n",
    "        epes = []\n",
    "        f1alls = []\n",
    "        for index, filename in model_pred_map.items():\n",
    "            epe = EndPointError()\n",
    "            pred_flow, pred_valid = load_kitti_flow(filename)\n",
    "            gt_flow, gt_valid = gt_map[index]\n",
    "            \n",
    "            gt_flow = gt_flow.astype(np.float32)\n",
    "            pred_flow = pred_flow.astype(np.float32)\n",
    "\n",
    "            diff1 = np.linalg.norm(pred_flow - gt_flow, axis=-1)\n",
    "            diff2 = np.linalg.norm(pred_flow - gt_flow[:,:,::-1], axis=-1)\n",
    "                    \n",
    "            score1 = np.mean(diff1[gt_valid])\n",
    "            score2 = np.mean(diff2[gt_valid])\n",
    "\n",
    "            # if score1 <= score2:\n",
    "            score = score1      \n",
    "            f1all = 100 * np.count_nonzero(diff1[gt_valid] >= np.clip(0.05 * np.linalg.norm(gt_flow[gt_valid], axis=-1),3, None)) / np.count_nonzero(gt_valid)\n",
    "            # else:\n",
    "            #     tqdm.write(f'Flip {model_name} {index}')\n",
    "            #     score = score2\n",
    "            #     f1all = 100 * np.count_nonzero(diff2[gt_valid] >= 3) / np.count_nonzero(gt_valid)\n",
    "            \n",
    "            epes.append(score)\n",
    "            f1alls.append(f1all)\n",
    "        \n",
    "        results.append({'Model': model, 'Checkpoint': ckpt, 'Mean EPE': np.mean(epes), 'Mean F1-All': np.mean(f1alls)})\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = batch_eval(r\"./data_kitti\", r\"./results/inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d282c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.sort_values(\"Mean EPE\", inplace=True)\n",
    "df['Checkpoint'].replace('mixed','mix', inplace=True)\n",
    "# df = df[~df['Checkpoint'].str.contains(\"mix\")]\n",
    "df['Checkpoint'] = df['Checkpoint'].str.capitalize()\n",
    "# df['Mean F1-All'] = df['Mean F1-All'].map('{:.2f}%'.format)\n",
    "df.rename(columns={'Mean F1-All':'F1-All','Mean EPE': 'EPE'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50235862",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = df.pivot(index='Model', columns='Checkpoint')\n",
    "pivot_df = pivot_df.swaplevel(axis=1).sort_index(axis=1, level=0)\n",
    "# pivot_df.sort_values((\"Things\",\"EPE\"), inplace=True)\n",
    "pivot_df = pivot_df.loc[pivot_df.xs(\"EPE\", level=1, axis=1).mean(axis=1, skipna=True).sort_values().index]\n",
    "\n",
    "sorted_col0 = pivot_df.xs(\"EPE\", level=1, axis=1).notna().sum().sort_values(ascending=False).index.tolist()\n",
    "pivot_df = pivot_df.loc[:, sum([[col0 for col0 in pivot_df.columns if col0[0] == k] for k in sorted_col0], [])]\n",
    "\n",
    "\n",
    "# pivot_df.fillna('-',inplace=True)\n",
    "print(pivot_df)\n",
    "pivot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9caf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = pivot_df.to_latex(index=True, escape=True, float_format=\"%.4f\",\n",
    "                            caption=\"Mean End-Point Error (EPE) for different models, sorted ascendingly.\",\n",
    "                            label=\"tab:epe_results\",\n",
    "                            multicolumn=True,\n",
    "                            multicolumn_format='c',\n",
    "                            na_rep='-',\n",
    "                            longtable=True)\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.to_excel('results/data/epe_evaluation.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7eb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
